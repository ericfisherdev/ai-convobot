name: Release Builds

on:
  push:
    tags:
      - 'v*'
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      version:
        description: 'Release version (e.g., v1.0.0)'
        required: true
        default: 'v1.0.0'

env:
  CARGO_TERM_COLOR: always

jobs:
  build-releases:
    name: Build Release Binaries
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          # Windows builds
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            build_type: cpu
            artifact_name: ai-companion-windows-x64-cpu
            executable_suffix: .exe
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            build_type: cuda
            artifact_name: ai-companion-windows-x64-cuda
            executable_suffix: .exe
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            build_type: opencl
            artifact_name: ai-companion-windows-x64-opencl
            executable_suffix: .exe

          # Linux builds
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            build_type: cpu
            artifact_name: ai-companion-linux-x64-cpu
            executable_suffix: ""
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            build_type: cuda
            artifact_name: ai-companion-linux-x64-cuda
            executable_suffix: ""
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            build_type: opencl
            artifact_name: ai-companion-linux-x64-opencl
            executable_suffix: ""

          # macOS builds
          - os: macos-latest
            target: x86_64-apple-darwin
            build_type: cpu
            artifact_name: ai-companion-macos-x64-cpu
            executable_suffix: ""
          - os: macos-latest
            target: x86_64-apple-darwin
            build_type: metal
            artifact_name: ai-companion-macos-x64-metal
            executable_suffix: ""
          - os: macos-latest
            target: aarch64-apple-darwin
            build_type: cpu
            artifact_name: ai-companion-macos-arm64-cpu
            executable_suffix: ""
          - os: macos-latest
            target: aarch64-apple-darwin
            build_type: metal
            artifact_name: ai-companion-macos-arm64-metal
            executable_suffix: ""

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
          target: ${{ matrix.target }}
          components: rustfmt, clippy
          rustflags: ""

      # Windows-specific setup
      - name: Setup Windows Build Environment
        if: runner.os == 'Windows'
        shell: powershell
        run: |
          # Install Visual Studio Build Tools if needed
          choco install visualstudio2022buildtools --package-parameters "--add Microsoft.VisualStudio.Workload.VCTools"
          
          # Set up CUDA for Windows builds
          if ("${{ matrix.build_type }}" -eq "cuda") {
            Write-Host "Setting up CUDA for Windows build..."
            
            # Try to install CUDA toolkit on Windows
            try {
              # Download and install CUDA toolkit 12.2
              Write-Host "Downloading CUDA toolkit installer..."
              $cudaUrl = "https://developer.download.nvidia.com/compute/cuda/12.2.2/network_installers/cuda_12.2.2_windows_network.exe"
              $cudaInstaller = "$env:TEMP\cuda_installer.exe"
              Invoke-WebRequest -Uri $cudaUrl -OutFile $cudaInstaller -UseBasicParsing
              
              Write-Host "Installing CUDA toolkit (silent install with timeout)..."
              $process = Start-Process -FilePath $cudaInstaller -ArgumentList "/S" -PassThru -NoNewWindow
              if (-not $process.WaitForExit(600000)) { # 10 minute timeout
                $process.Kill()
                throw "CUDA installation timed out after 10 minutes"
              }
              if ($process.ExitCode -ne 0) {
                throw "CUDA installer exited with code $($process.ExitCode)"
              }
              
              # Set CUDA environment variables
              $cudaPath = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.2"
              if (Test-Path $cudaPath) {
                echo "CUDA_PATH=$cudaPath" | Out-File -FilePath $env:GITHUB_ENV -Append
                echo "CUDA_HOME=$cudaPath" | Out-File -FilePath $env:GITHUB_ENV -Append
                echo "PATH=$cudaPath\bin;$env:PATH" | Out-File -FilePath $env:GITHUB_ENV -Append
                echo "CUDA_AVAILABLE=true" | Out-File -FilePath $env:GITHUB_ENV -Append
                Write-Host "CUDA toolkit installed and configured successfully"
              } else {
                throw "CUDA installation directory not found"
              }
              
            } catch {
              Write-Host "CUDA installation failed: $_"
              Write-Host "Falling back to CPU-only build"
              echo "CUDA_AVAILABLE=false" | Out-File -FilePath $env:GITHUB_ENV -Append
            }
          } else {
            echo "CUDA_AVAILABLE=false" | Out-File -FilePath $env:GITHUB_ENV -Append
          }
          
          if ("${{ matrix.build_type }}" -eq "opencl") {
            # For Windows OpenCL, assume availability
            echo "OPENCL_AVAILABLE=true" | Out-File -FilePath $env:GITHUB_ENV -Append
            Write-Host "Windows OpenCL build requested - assuming OpenCL availability"
          } else {
            echo "OPENCL_AVAILABLE=false" | Out-File -FilePath $env:GITHUB_ENV -Append
          }

      # Linux-specific setup
      - name: Setup Linux Build Environment
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential pkg-config libssl-dev
          
          # Install CUDA toolkit for CUDA builds (with proper error handling)
          if [ "${{ matrix.build_type }}" = "cuda" ]; then
            echo "Setting up CUDA environment..."
            
            # Set up proper environment first
            export DEBIAN_FRONTEND=noninteractive
            
            # Install CUDA toolkit with more robust error handling
            {
              echo "Downloading CUDA keyring..."
              wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
              
              echo "Installing CUDA keyring..."
              sudo dpkg -i cuda-keyring_1.0-1_all.deb
              
              echo "Updating package lists..."
              sudo apt-get update -qq
              
              echo "Installing CUDA toolkit (this may take several minutes)..."
              sudo apt-get install -y --no-install-recommends \
                cuda-toolkit-12-2 \
                cuda-compiler-12-2 \
                cuda-libraries-dev-12-2 \
                cuda-driver-dev-12-2
              
              # Verify CUDA installation
              export PATH="/usr/local/cuda-12.2/bin:$PATH"
              export LD_LIBRARY_PATH="/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH"
              export CUDA_PATH="/usr/local/cuda-12.2"
              export CUDA_ROOT="/usr/local/cuda-12.2"
              export CUDA_HOME="/usr/local/cuda-12.2"
              export CUDA_INCLUDE_PATH="/usr/local/cuda-12.2/include"
              export CUDA_LIB_PATH="/usr/local/cuda-12.2/lib64"
              
              # Add to GitHub environment for subsequent steps
              echo "PATH=/usr/local/cuda-12.2/bin:$PATH" >> $GITHUB_ENV
              echo "LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH" >> $GITHUB_ENV
              echo "CUDA_PATH=/usr/local/cuda-12.2" >> $GITHUB_ENV
              echo "CUDA_ROOT=/usr/local/cuda-12.2" >> $GITHUB_ENV
              echo "CUDA_HOME=/usr/local/cuda-12.2" >> $GITHUB_ENV
              echo "CUDA_INCLUDE_PATH=/usr/local/cuda-12.2/include" >> $GITHUB_ENV
              echo "CUDA_LIB_PATH=/usr/local/cuda-12.2/lib64" >> $GITHUB_ENV
              
              # Verify nvcc is available
              if command -v nvcc >/dev/null 2>&1; then
                nvcc --version
                echo "CUDA toolkit installed and verified successfully"
                echo "CUDA_AVAILABLE=true" >> $GITHUB_ENV
              else
                echo "NVCC not found in PATH, CUDA installation may have failed"
                echo "CUDA_AVAILABLE=false" >> $GITHUB_ENV
                exit 1
              fi
              
            } || {
              echo "CUDA installation failed, will build CPU-only version instead"
              echo "Warning: CUDA build requested but CUDA installation failed"
              echo "CUDA_AVAILABLE=false" >> $GITHUB_ENV
            }
          else
            echo "CUDA_AVAILABLE=false" >> $GITHUB_ENV
          fi
          
          # Install OpenCL for OpenCL builds (with better error handling)
          if [ "${{ matrix.build_type }}" = "opencl" ]; then
            echo "Setting up OpenCL environment..."
            {
              sudo apt-get install -y opencl-headers ocl-icd-opencl-dev
              echo "OpenCL headers installed successfully"
              echo "OPENCL_AVAILABLE=true" >> $GITHUB_ENV
            } || {
              echo "OpenCL installation failed, will build CPU-only version instead"
              echo "OPENCL_AVAILABLE=false" >> $GITHUB_ENV
            }
          else
            echo "OPENCL_AVAILABLE=false" >> $GITHUB_ENV
          fi

      # macOS-specific setup
      - name: Setup macOS Build Environment
        if: runner.os == 'macOS'
        run: |
          # Install required dependencies
          brew install pkg-config openssl
          
          # For cross-compilation to ARM64
          if [ "${{ matrix.target }}" = "aarch64-apple-darwin" ]; then
            rustup target add aarch64-apple-darwin
          fi
          
          # Set up GPU availability flags for macOS
          # Metal is generally available on all modern Macs
          if [ "${{ matrix.build_type }}" = "metal" ]; then
            echo "METAL_AVAILABLE=true" >> $GITHUB_ENV
            echo "macOS Metal build - Metal should be available on modern Macs"
          fi
          
          # macOS doesn't typically use CUDA or OpenCL in our builds
          echo "CUDA_AVAILABLE=false" >> $GITHUB_ENV
          echo "OPENCL_AVAILABLE=false" >> $GITHUB_ENV

      - name: Install frontend dependencies
        run: npm install

      - name: Build frontend
        run: npm run build
        env:
          NODE_ENV: production

      - name: Set build features
        id: features
        shell: bash
        run: |
          case "${{ matrix.build_type }}" in
            "cuda")
              if [ "${CUDA_AVAILABLE:-false}" = "true" ]; then
                echo "features=cublas" >> $GITHUB_OUTPUT
                echo "build_name=CUDA" >> $GITHUB_OUTPUT
              else
                echo "features=" >> $GITHUB_OUTPUT
                echo "build_name=CPU (CUDA unavailable)" >> $GITHUB_OUTPUT
              fi
              ;;
            "opencl")
              if [ "${OPENCL_AVAILABLE:-false}" = "true" ]; then
                echo "features=clblast" >> $GITHUB_OUTPUT
                echo "build_name=OpenCL" >> $GITHUB_OUTPUT
              else
                echo "features=" >> $GITHUB_OUTPUT
                echo "build_name=CPU (OpenCL unavailable)" >> $GITHUB_OUTPUT
              fi
              ;;
            "metal")
              if [ "${METAL_AVAILABLE:-true}" = "true" ]; then
                echo "features=metal" >> $GITHUB_OUTPUT
                echo "build_name=Metal" >> $GITHUB_OUTPUT
              else
                echo "features=" >> $GITHUB_OUTPUT
                echo "build_name=CPU (Metal unavailable)" >> $GITHUB_OUTPUT
              fi
              ;;
            *)
              echo "features=" >> $GITHUB_OUTPUT
              echo "build_name=CPU" >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Build backend
        shell: bash
        run: |
          cd backend
          
          # Set environment variables for cross-compilation
          if [ "${{ matrix.target }}" = "aarch64-apple-darwin" ] && [ "${{ runner.os }}" = "macOS" ]; then
            export MACOSX_DEPLOYMENT_TARGET=11.0
            export SDKROOT=$(xcrun --sdk macosx --show-sdk-path)
          fi
          
          # Allow warnings in release builds
          export RUSTFLAGS=""
          
          # Build with appropriate features (already determined by feature availability check)
          if [ -n "${{ steps.features.outputs.features }}" ]; then
            echo "Building ${{ steps.features.outputs.build_name }} version with features: ${{ steps.features.outputs.features }}"
            cargo build --release --target ${{ matrix.target }} --features ${{ steps.features.outputs.features }}
          else
            echo "Building ${{ steps.features.outputs.build_name }} version (CPU-only)"
            cargo build --release --target ${{ matrix.target }}
          fi

      - name: Create release package
        shell: bash
        run: |
          # Create package directory
          mkdir -p release-package
          
          # Copy binary
          binary_name="ai-companion${{ matrix.executable_suffix }}"
          cp "backend/target/${{ matrix.target }}/release/${binary_name}" "release-package/"
          
          # Create README for the release
          cat > release-package/README.md << 'EOF'
          # AI Companion - ${{ steps.features.outputs.build_name }} Build
          
          ## About
          This is a release build of AI Companion with ${{ steps.features.outputs.build_name }} acceleration support.
          
          ## System Requirements
          - Operating System: ${{ runner.os }}
          - Architecture: ${{ matrix.target }}
          - Acceleration: ${{ steps.features.outputs.build_name }}
          
          ### Additional Requirements for ${{ steps.features.outputs.build_name }} builds:
          EOF
          
          # Add specific requirements based on build type
          case "${{ matrix.build_type }}" in
            "cuda")
              cat >> release-package/README.md << 'EOF'
          - NVIDIA GPU with CUDA Compute Capability 3.5 or higher
          - CUDA Runtime 12.0 or later
          - NVIDIA Graphics Drivers 525.60.13 or later (Linux) / 528.02 or later (Windows)
          EOF
              ;;
            "opencl")
              cat >> release-package/README.md << 'EOF'
          - OpenCL 1.2 compatible GPU (NVIDIA, AMD, or Intel)
          - OpenCL runtime libraries installed
          - Updated GPU drivers
          EOF
              ;;
            "metal")
              cat >> release-package/README.md << 'EOF'
          - macOS 10.15 (Catalina) or later
          - Metal-compatible GPU (most modern Macs)
          - No additional drivers required
          EOF
              ;;
            "cpu")
              cat >> release-package/README.md << 'EOF'
          - No additional GPU requirements
          - Runs on CPU only
          EOF
              ;;
          esac
          
          cat >> release-package/README.md << 'EOF'
          
          ## Usage
          1. Extract the package to a directory of your choice
          2. Run the executable: `./ai-companion` (Linux/macOS) or `ai-companion.exe` (Windows)
          3. Open your browser to `http://localhost:3000`
          4. Configure your LLM model path in the settings
          5. Start chatting with your AI companion!
          
          ## Configuration
          - The application will create a `companion_database.db` file in the current directory
          - Default port is 3000 (configurable in the application)
          - LLM models must be in GGUF format
          
          ## GPU Memory Management
          This build includes dynamic GPU layer allocation:
          - Automatically detects available VRAM
          - Optimizes GPU layer allocation based on memory
          - Provides fallback to CPU if GPU memory is insufficient
          - Configurable safety margins and allocation strategies
          
          ## Support
          For issues and documentation, visit: https://github.com/ericfisherdev/ai-convobot
          EOF
          
          # Create version info
          echo "${{ github.event.inputs.version || github.ref_name }}" > release-package/VERSION
          
          # Create a simple launcher script for Unix systems
          if [ "${{ runner.os }}" != "Windows" ]; then
            cat > release-package/start.sh << 'EOF'
          #!/bin/bash
          # AI Companion Launcher Script
          
          echo "Starting AI Companion..."
          echo "Build: ${{ steps.features.outputs.build_name }}"
          echo "Target: ${{ matrix.target }}"
          echo ""
          
          # Check if model file exists
          if [ ! -f "companion_database.db" ]; then
              echo "First run detected. The application will create necessary files."
              echo ""
          fi
          
          # Start the application
          ./ai-companion
          EOF
            chmod +x release-package/start.sh
          fi
          
          # Create batch file for Windows
          if [ "${{ runner.os }}" = "Windows" ]; then
            cat > release-package/start.bat << 'EOF'
          @echo off
          echo Starting AI Companion...
          echo Build: ${{ steps.features.outputs.build_name }}
          echo Target: ${{ matrix.target }}
          echo.
          
          REM Check if database exists
          if not exist "companion_database.db" (
              echo First run detected. The application will create necessary files.
              echo.
          )
          
          REM Start the application
          ai-companion.exe
          pause
          EOF
          fi

      - name: Create archive
        shell: bash
        run: |
          cd release-package
          
          if [ "${{ runner.os }}" = "Windows" ]; then
            # Create ZIP for Windows
            7z a "../${{ matrix.artifact_name }}.zip" *
          else
            # Create tar.gz for Unix systems
            tar -czf "../${{ matrix.artifact_name }}.tar.gz" *
          fi

      - name: Upload release artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.artifact_name }}
          path: |
            ${{ matrix.artifact_name }}.zip
            ${{ matrix.artifact_name }}.tar.gz
          retention-days: 30

  create-release:
    name: Create GitHub Release
    needs: build-releases
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Prepare release assets
        run: |
          mkdir -p release-assets
          find ./artifacts -name "*.zip" -o -name "*.tar.gz" | while read file; do
            cp "$file" release-assets/
          done
          ls -la release-assets/

      - name: Generate release notes
        id: release_notes
        run: |
          cat > release_notes.md << 'EOF'
          # AI Companion Release ${{ github.ref_name }}
          
          ## Features
          - Dynamic GPU Layer Allocation with intelligent VRAM management
          - Support for CUDA, OpenCL, and Metal acceleration
          - Real-time GPU memory monitoring and optimization
          - Comprehensive attitude tracking and management system
          - Advanced context management and token optimization
          - Character card import support (JSON and PNG formats)
          - Long-term memory with semantic search capabilities
          
          ## Available Builds
          
          ### Windows (x64)
          - **CPU Only**: `ai-companion-windows-x64-cpu.zip` - Compatible with all Windows systems
          - **CUDA**: `ai-companion-windows-x64-cuda.zip` - Requires NVIDIA GPU with CUDA support
          - **OpenCL**: `ai-companion-windows-x64-opencl.zip` - Compatible with NVIDIA, AMD, and Intel GPUs
          
          ### Linux (x64)
          - **CPU Only**: `ai-companion-linux-x64-cpu.tar.gz` - Compatible with all Linux systems
          - **CUDA**: `ai-companion-linux-x64-cuda.tar.gz` - Requires NVIDIA GPU with CUDA support
          - **OpenCL**: `ai-companion-linux-x64-opencl.tar.gz` - Compatible with NVIDIA, AMD, and Intel GPUs
          
          ### macOS
          - **Intel CPU Only**: `ai-companion-macos-x64-cpu.tar.gz` - For Intel-based Macs
          - **Intel + Metal**: `ai-companion-macos-x64-metal.tar.gz` - For Intel Macs with Metal support
          - **Apple Silicon CPU**: `ai-companion-macos-arm64-cpu.tar.gz` - For M1/M2/M3 Macs
          - **Apple Silicon + Metal**: `ai-companion-macos-arm64-metal.tar.gz` - For M1/M2/M3 Macs with Metal acceleration
          
          ## Quick Start
          1. Download the appropriate build for your system
          2. Extract the archive
          3. Run the executable or use the provided launcher script
          4. Open `http://localhost:3000` in your browser
          5. Configure your GGUF model path in settings
          6. Start chatting!
          
          ## GPU Memory Management
          This release includes intelligent GPU layer allocation:
          - Automatically detects available VRAM
          - Calculates optimal GPU layer distribution
          - Provides real-time memory monitoring
          - Supports multiple allocation strategies
          - Graceful fallback to CPU when needed
          
          ## System Requirements
          - **Minimum RAM**: 4GB (8GB recommended)
          - **Storage**: 500MB free space (plus space for LLM models)
          - **Models**: GGUF format LLM models
          - **GPU** (optional): See build-specific requirements above
          
          ## What's New in This Release
          - Dynamic GPU layer allocation system
          - Enhanced memory management and monitoring
          - Improved frontend interface with GPU status display
          - Better error handling and fallback mechanisms
          - Performance optimizations and bug fixes
          
          For detailed documentation and support, visit: https://github.com/ericfisherdev/ai-convobot
          EOF

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          name: AI Companion ${{ github.ref_name }}
          body_path: release_notes.md
          files: release-assets/*
          draft: false
          prerelease: ${{ contains(github.ref_name, 'alpha') || contains(github.ref_name, 'beta') || contains(github.ref_name, 'rc') }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}